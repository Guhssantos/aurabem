# -*- coding: utf-8 -*-
import streamlit as st
import google.generativeai as genai
import time
import re
import logging
import os

# --- Bloco de Configura√ß√£o Inicial ---
# Logger de depura√ß√£o inicial (opcional, mas √∫til durante o desenvolvimento)
debug_logger = logging.getLogger("startup_debug")
stream_handler_debug = logging.StreamHandler()
formatter_debug = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
stream_handler_debug.setFormatter(formatter_debug)
if not debug_logger.handlers: # Evita adicionar handlers duplicados em reruns do Streamlit
    debug_logger.addHandler(stream_handler_debug)
debug_logger.setLevel(logging.DEBUG)
debug_logger.info("Iniciando script app.py para Aura Bem...")


# Configura√ß√£o do Logging Principal do Aplicativo
logging.basicConfig(
    level=logging.INFO, # Mude para logging.DEBUG para ver mais detalhes
    format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# --- Constantes ---
SESSION_MESSAGES_KEY = "aura_bem_messages"
SESSION_CHAT_KEY = "aura_bem_chat_session"
SYSTEM_PROMPT_FILENAME = "system_prompt_aura_bem.txt" # Nome do arquivo de prompt
DEFAULT_SYSTEM_INSTRUCTION = (
    "Voc√™ √© Aura Bem, uma IA de apoio. Seu objetivo √© ouvir com empatia. "
    "Avise que voc√™ √© uma IA e n√£o uma psic√≥loga. "
    "Se o arquivo de personalidade completo n√£o p√¥de ser carregado, pe√ßa desculpas."
)

# --- Bloco 1: Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Aura Bem - Sua Companheira de Bem-Estar",
    page_icon="üíñ",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- Bloco 2: T√≠tulo e Descri√ß√£o ---
st.title("üíñ Aura Bem: Sua Companheira de Bem-Estar")
st.caption(
    "Um espa√ßo seguro para conversar, encontrar acolhimento e explorar seus sentimentos. "
    "Lembre-se, sou Aura Bem, uma IA, e n√£o substituo o aconselhamento de um psic√≥logo profissional."
)
st.divider()

# --- Bloco 3: Carregamento da System Instruction e Configura√ß√£o da API ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ABSOLUTE_PATH_TO_PROMPT_FILE = os.path.join(SCRIPT_DIR, SYSTEM_PROMPT_FILENAME)
debug_logger.info(f"Tentando carregar prompt de: {ABSOLUTE_PATH_TO_PROMPT_FILE}")

system_instruction_aura_bem = ""
critical_error_loading_prompt = False

try:
    if os.path.exists(ABSOLUTE_PATH_TO_PROMPT_FILE):
        with open(ABSOLUTE_PATH_TO_PROMPT_FILE, "r", encoding="utf-8") as f:
            system_instruction_aura_bem = f.read()
        logger.info(f"Instru√ß√£o do sistema '{SYSTEM_PROMPT_FILENAME}' carregada com sucesso.")
        if not system_instruction_aura_bem.strip():
            logger.warning(f"Arquivo '{SYSTEM_PROMPT_FILENAME}' est√° vazio. Usando fallback.")
            system_instruction_aura_bem = DEFAULT_SYSTEM_INSTRUCTION
            st.warning(f"Aten√ß√£o: Arquivo de personalidade da Aura Bem ('{SYSTEM_PROMPT_FILENAME}') est√° vazio. Usando configura√ß√£o padr√£o.")
    else:
        logger.error(f"ERRO CR√çTICO: Arquivo '{SYSTEM_PROMPT_FILENAME}' n√£o encontrado em '{ABSOLUTE_PATH_TO_PROMPT_FILE}'.")
        debug_logger.error(f"Conte√∫do do diret√≥rio '{SCRIPT_DIR}': {os.listdir(SCRIPT_DIR) if os.path.exists(SCRIPT_DIR) else 'Diret√≥rio n√£o encontrado'}")
        st.error(
            f"Erro Cr√≠tico de Configura√ß√£o: O arquivo de personalidade da Aura Bem ('{SYSTEM_PROMPT_FILENAME}') "
            f"n√£o foi encontrado. A Aura Bem usar√° uma configura√ß√£o b√°sica. Verifique o deployment."
        )
        system_instruction_aura_bem = DEFAULT_SYSTEM_INSTRUCTION
        critical_error_loading_prompt = True
except Exception as e:
    logger.error(f"Erro ao ler '{ABSOLUTE_PATH_TO_PROMPT_FILE}': {e}", exc_info=True)
    st.error(f"Erro ao carregar personalidade da Aura Bem: {e}. Usando configura√ß√£o b√°sica.")
    system_instruction_aura_bem = DEFAULT_SYSTEM_INSTRUCTION
    critical_error_loading_prompt = True

# Configura√ß√£o da API Key do Google
try:
    GOOGLE_API_KEY_APP = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY_APP)
    logger.info("Chave API do Google configurada com sucesso.")
except KeyError:
    logger.critical("Chave API do Google (GOOGLE_API_KEY) N√ÉO encontrada nos Secrets do Streamlit.")
    st.error("ERRO GRAVE: A Chave API do Google n√£o foi configurada nos 'Secrets' do Streamlit. O aplicativo n√£o pode se conectar √† IA. Por favor, configure-a.")
    st.stop()
except Exception as e:
    logger.critical(f"Erro inesperado e grave ao configurar a API Key: {e}", exc_info=True)
    st.error(f"Erro grave ao configurar a API Key: {e}. O aplicativo n√£o pode continuar.")
    st.stop()

# --- Bloco 4: Configura√ß√£o do Modelo Gemini ---
generation_config = {
    "temperature": 0.65,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 1024,
}
safety_settings = [
    {"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
    for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]
]

# --- Bloco 6: Defini√ß√µes de Seguran√ßa (CVV) e Detec√ß√£o de Risco ---
keywords_risco_originais = [
    "me matar", "me mate", "suicidio", "suic√≠dio", "n√£o aguento mais viver", "quero morrer",
    "queria morrer", "quero sumir", "desistir de tudo", "acabar com tudo", "fazer mal a mim",
    "me cortar", "me machucar", "automutila√ß√£o", "quero me jogar", "tirar minha vida",
    "sem esperan√ßa", "fim da linha"
]
keywords_risco_regex = [re.compile(r'\b' + re.escape(kw) + r'\b', re.IGNORECASE) for kw in keywords_risco_originais]
resposta_risco_padrao = (
    "Sinto muito que voc√™ esteja passando por um momento t√£o dif√≠cil e pensando nisso. "
    "√â muito importante buscar ajuda profissional **imediatamente**. Por favor, entre em contato com o "
    "**CVV (Centro de Valoriza√ß√£o da Vida) ligando para o n√∫mero 188**. A liga√ß√£o √© gratuita "
    "e eles est√£o dispon√≠veis 24 horas por dia para conversar com voc√™ de forma sigilosa e segura. "
    "Voc√™ n√£o est√° sozinho(a) e h√° pessoas prontas para te ouvir e ajudar. Por favor, ligue para eles agora. üôè"
)

# --- Bloco 7: Fun√ß√£o para Inicializar o Modelo ---
@st.cache_resource
def init_model(instruction: str):
    if instruction == DEFAULT_SYSTEM_INSTRUCTION:
        logger.warning("Inicializando modelo com instru√ß√£o de sistema DE FALLBACK.")
    else:
        logger.info(f"Inicializando modelo com system prompt personalizado '{SYSTEM_PROMPT_FILENAME}'.")
    try:
        model_instance = genai.GenerativeModel(
            model_name="gemini-1.5-flash-latest",
            generation_config=generation_config,
            safety_settings=safety_settings,
            system_instruction=instruction
        )
        return model_instance
    except Exception as e_model:
        logger.critical(f"Erro GRAVE ao carregar o modelo de IA: {e_model}", exc_info=True)
        st.error(f"Erro grave ao carregar o modelo de IA: {e_model}. O app n√£o pode continuar.")
        st.stop()

model = init_model(system_instruction_aura_bem)

# --- Bloco 8: Gerenciamento do Hist√≥rico da Conversa e Bot√£o de Reset ---
if SESSION_MESSAGES_KEY in st.session_state and len(st.session_state[SESSION_MESSAGES_KEY]) > 1:
    if st.sidebar.button("üßπ Limpar Conversa Atual"):
        initial_message_reset = "Ol√°! Sou Aura Bem. Como posso te ajudar a se sentir um pouco melhor hoje? (Conversa reiniciada)"
        if critical_error_loading_prompt:
            initial_message_reset = (
                "Ol√°! Sou Aura Bem. Minha configura√ß√£o de personalidade completa n√£o carregou. "
                "Farei o meu melhor com uma configura√ß√£o b√°sica. (Conversa reiniciada)"
            )
        st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": initial_message_reset}]
        if SESSION_CHAT_KEY in st.session_state: del st.session_state[SESSION_CHAT_KEY]
        logger.info("Hist√≥rico da conversa e sess√£o do Gemini reiniciados pelo usu√°rio.")
        st.rerun()

if SESSION_MESSAGES_KEY not in st.session_state:
    initial_message_load = "Ol√°! Sou Aura Bem. Sinta-se √† vontade para compartilhar como voc√™ est√° se sentindo. Estou aqui para ouvir. üòä"
    if critical_error_loading_prompt:
        initial_message_load = (
            "Ol√°! Sou Aura Bem. Parece que tive um problema ao carregar minha configura√ß√£o completa de personalidade. "
            "Vou tentar te ajudar da melhor forma poss√≠vel com uma configura√ß√£o b√°sica, mas pe√ßo desculpas se minha intera√ß√£o n√£o for a ideal. "
            "Como voc√™ est√° se sentindo hoje?"
        )
    st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": initial_message_load}]
    logger.info(f"Hist√≥rico de mensagens inicializado. Erro cr√≠tico no prompt: {critical_error_loading_prompt}")

# --- Bloco 9: Exibi√ß√£o do Hist√≥rico ---
for message in st.session_state[SESSION_MESSAGES_KEY]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Bloco 10: Fun√ß√£o para Enviar Mensagem e Processar Resposta ---
def send_message_to_aura_bem(user_prompt: str):
    st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": user_prompt})
    with st.chat_message("user"): st.markdown(user_prompt)

    bot_response_final = ""
    try:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response_stream = ""
            
            if SESSION_CHAT_KEY not in st.session_state:
                history_for_model = [{"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]}
                                     for m in st.session_state[SESSION_MESSAGES_KEY][:-1]]
                st.session_state[SESSION_CHAT_KEY] = model.start_chat(history=history_for_model)
                logger.info(f"Nova sess√£o de chat Aura Bem iniciada com {len(history_for_model)} msgs de hist√≥rico.")

            with st.spinner("Aura Bem est√° refletindo... ü§î"):
                response_stream = st.session_state[SESSION_CHAT_KEY].send_message(user_prompt, stream=True)

            last_chunk = None
            for chunk in response_stream:
                last_chunk = chunk
                if chunk.parts:
                    for part in chunk.parts: full_response_stream += part.text
                elif hasattr(chunk, 'text') and chunk.text: full_response_stream += chunk.text
                message_placeholder.markdown(full_response_stream + "‚ñå")

                if chunk.prompt_feedback and chunk.prompt_feedback.block_reason:
                    logger.warning(f"Resposta Aura Bem parcialmente gerada e bloqueada: {chunk.prompt_feedback.block_reason}")
                    full_response_stream += f"\n\n*(Minha resposta foi interrompida para manter nossa conversa segura. Poderia tentar de outra forma?)*"
                    message_placeholder.warning(full_response_stream.strip())
                    break
            
            bot_response_final = full_response_stream.strip()
            if not (last_chunk and last_chunk.prompt_feedback and last_chunk.prompt_feedback.block_reason):
                message_placeholder.markdown(bot_response_final)

            is_blocked_final = last_chunk and last_chunk.prompt_feedback and last_chunk.prompt_feedback.block_reason
            if is_blocked_final and not bot_response_final:
                reason = f"{last_chunk.prompt_feedback.block_reason}"
                logger.error(f"Prompt para Aura Bem totalmente bloqueado: {reason}")
                bot_response_final = f"Sua mensagem n√£o p√¥de ser processada ({reason}). Vamos tentar de outra forma?"
                message_placeholder.error(bot_response_final)
            elif not bot_response_final and not is_blocked_final:
                logger.warning(f"Resposta vazia da Aura Bem para o prompt: '{user_prompt}'. Usando fallback.")
                bot_response_final = "Pe√ßo desculpas, parece que n√£o encontrei as palavras certas para responder a isso. Poderia me dizer de outra forma ou explorar outro sentimento?"
                message_placeholder.markdown(bot_response_final)

        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": bot_response_final})

    except genai.types.BlockedPromptException as bpe:
        logger.error(f"BlockedPromptException (Aura Bem): {bpe}", exc_info=True)
        err_msg = "Sua mensagem foi bloqueada por diretrizes de seguran√ßa. Poderia tentar de uma forma diferente, por favor?"
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": err_msg})
        with st.chat_message("assistant"): st.error(err_msg)
    except Exception as e:
        logger.error(f"Falha ao enviar/processar mensagem (Aura Bem): {e}", exc_info=True)
        err_msg = "Desculpe, ocorreu um pequeno contratempo t√©cnico da minha parte. Poderia tentar novamente, por favor? üòî"
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": err_msg})
        with st.chat_message("assistant"): st.markdown(err_msg)

# --- Bloco 11: Input e L√≥gica Principal ---
if prompt := st.chat_input("Como voc√™ est√° se sentindo hoje?..."):
    logger.info(f"Usu√°rio (Aura Bem) enviou: '{prompt[:70]}...'")
    if any(regex.search(prompt) for regex in keywords_risco_regex):
        logger.warning(f"RISCO DETECTADO (Aura Bem) na mensagem: '{prompt}'")
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        with st.chat_message("assistant"):
            st.warning("**Aten√ß√£o:** Se voc√™ est√° pensando em se machucar ou sente que sua seguran√ßa est√° em risco, √© muito importante buscar ajuda profissional imediatamente.")
            st.markdown(resposta_risco_padrao)
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": resposta_risco_padrao})
    else:
        send_message_to_aura_bem(prompt)

# Bloco 12 (Coleta de Feedback) foi REMOVIDO

# --- Bloco 13: Recursos Adicionais (Sidebar) ---
with st.sidebar:
    st.markdown("---")
    st.subheader("Apoio Adicional üåª")
    with st.expander("Quando buscar ajuda profissional:", expanded=True):
        st.markdown(
            "Lembre-se, Aura Bem √© uma IA para escuta e apoio, mas **n√£o substitui um psic√≥logo.** "
            "Se voc√™ sente que precisa de um acompanhamento mais profundo, ou se o sofrimento emocional "
            "est√° impactando seu dia a dia, considere procurar um profissional de sa√∫de mental. "
            "Eles podem oferecer o suporte especializado que voc√™ merece."
        )
    with st.expander("Contato de Emerg√™ncia (Brasil):", expanded=True):
        st.markdown("- **CVV (Centro de Valoriza√ß√£o da Vida):** Disque **188** (liga√ß√£o gratuita, dispon√≠vel 24h).")
        st.markdown("- **SAMU:** Disque **192** para emerg√™ncias m√©dicas.")
    with st.expander("Pequenas Pr√°ticas de Bem-Estar:", expanded=False):
        st.markdown(
            "*   **Respira√ß√£o Consciente:** Inspire contando at√© 4, segure por 4, expire contando at√© 6. Repita algumas vezes.\n"
            "*   **Di√°rio de Sentimentos:** Escrever sobre o que voc√™ sente pode ajudar a organizar os pensamentos.\n"
            "*   **Conex√£o Social:** Converse com um amigo ou familiar de confian√ßa.\n"
            "*   **Movimento Gentil:** Uma caminhada leve ou alongamentos podem liberar tens√µes."
        )

# --- Bloco 14: Rodap√© ---
st.divider()
st.caption(
    "Aura Bem √© uma Intelig√™ncia Artificial desenvolvida para oferecer escuta e apoio emocional. "
    "Ela **N√ÉO √© uma psic√≥loga** e suas sugest√µes n√£o substituem o aconselhamento profissional qualificado. "
    "Em caso de emerg√™ncia, ou se precisar de ajuda especializada, entre em contato com o **CVV (188)** ou procure um profissional de sa√∫de mental."
)
debug_logger.info("Finalizando script app.py para Aura Bem.")
# --- Fim do app.py ---
