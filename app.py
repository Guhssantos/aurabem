# -*- coding: utf-8 -*-
import streamlit as st
import google.generativeai as genai
import time
import re
import logging

# --- Bloco de ConfiguraÃ§Ã£o Inicial ---
# ConfiguraÃ§Ã£o do Logging (Melhorado e mais detalhado)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()  # Exibe logs no console
        # Considerar adicionar FileHandler para persistir logs em produÃ§Ã£o
        # logging.FileHandler("aura_app.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# --- Constantes para Session State (NOVO) ---
SESSION_MESSAGES_KEY = "aura_messages"
SESSION_CHAT_KEY = "aura_chat_session"
SESSION_FEEDBACK_SUBMITTED_KEY = "aura_feedback_submitted"

# --- Bloco 1: ConfiguraÃ§Ã£o da PÃ¡gina ---
st.set_page_config(
    page_title="Aura - Chatbot de Apoio",
    page_icon="ğŸ’–",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- Bloco 2: TÃ­tulo e DescriÃ§Ã£o ---
st.title("ğŸ’– Aura: Seu Companheiro Virtual")
st.caption("Um espaÃ§o seguro para conversar e encontrar acolhimento. Lembre-se, sou uma IA e nÃ£o posso substituir um terapeuta profissional.")
st.divider()

# --- Bloco 3: ConfiguraÃ§Ã£o da API Key e System Instruction ---
SYSTEM_PROMPT_FILE = "system_prompt_aura.txt"
system_instruction_aura = "" # Inicializa a variÃ¡vel

try:
    with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
        system_instruction_aura = f.read()
    logger.info(f"InstruÃ§Ã£o do sistema '{SYSTEM_PROMPT_FILE}' carregada com sucesso.")
except FileNotFoundError:
    logger.error(f"Arquivo de instruÃ§Ã£o do sistema '{SYSTEM_PROMPT_FILE}' nÃ£o encontrado! Usando um fallback.")
    st.error(f"Erro CrÃ­tico: O arquivo de configuraÃ§Ã£o da personalidade da Aura ({SYSTEM_PROMPT_FILE}) nÃ£o foi encontrado. A Aura pode nÃ£o funcionar como esperado.")
    system_instruction_aura = "VocÃª Ã© um chatbot de apoio. Seja gentil e prestativo. Avise que nÃ£o Ã© um terapeuta e que nÃ£o pode dar conselhos mÃ©dicos."
except Exception as e:
    logger.error(f"Erro ao carregar instruÃ§Ã£o do sistema '{SYSTEM_PROMPT_FILE}': {e}", exc_info=True)
    st.error(f"Erro ao carregar a personalidade da Aura: {e}. Usando fallback.")
    system_instruction_aura = "Falha ao carregar personalidade. Por favor, avise o desenvolvedor. Sou um chatbot de apoio, seja gentil."

# ConfiguraÃ§Ã£o da API Key do Google
try:
    GOOGLE_API_KEY_APP = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY_APP)
    logger.info("Chave API do Google configurada com sucesso via Streamlit Secrets.")
except KeyError:
    logger.error("Chave API do Google (GOOGLE_API_KEY) nÃ£o encontrada nos Secrets do Streamlit.")
    st.error("Ops! Parece que a Chave API do Google nÃ£o foi configurada nos 'Secrets' do Streamlit. PeÃ§a ajuda para configurÃ¡-la nas definiÃ§Ãµes do app.")
    st.stop()
except Exception as e:
    logger.error(f"Erro inesperado ao configurar a API Key: {e}", exc_info=True)
    st.error(f"Erro inesperado ao configurar a API Key: {e}")
    st.stop()

# --- Bloco 4: ConfiguraÃ§Ã£o do Modelo Gemini (MELHORIA) ---
generation_config = {
    "temperature": 0.7,  # Ligeiramente reduzida para respostas mais focadas e consistentes
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 800, # Aumentado um pouco para permitir respostas mais elaboradas se necessÃ¡rio
}
safety_settings = [
    {"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
    for c in [
        "HARM_CATEGORY_HARASSMENT",
        "HARM_CATEGORY_HATE_SPEECH",
        "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "HARM_CATEGORY_DANGEROUS_CONTENT",
    ]
]

# --- Bloco 5: (System Instruction jÃ¡ carregada no Bloco 3) ---

# --- Bloco 6: DefiniÃ§Ãµes de SeguranÃ§a (CVV) e DetecÃ§Ã£o de Risco Melhorada ---
keywords_risco_originais = [
    "me matar", "me mate", "suicidio", "suicÃ­dio",
    "nÃ£o aguento mais viver", "quero morrer", "queria morrer",
    "quero sumir", "desistir de tudo", "acabar com tudo",
    "fazer mal a mim", "me cortar", "me machucar", "automutilaÃ§Ã£o",
    "quero me jogar", "tirar minha vida", "sem esperanÃ§a", "fim da linha"
]
keywords_risco_regex = [
    re.compile(r'\b' + re.escape(keyword) + r'\b', re.IGNORECASE)
    for keyword in keywords_risco_originais
]
logger.info(f"{len(keywords_risco_regex)} padrÃµes de regex para detecÃ§Ã£o de risco compilados.")

resposta_risco_padrao = (
    "Sinto muito que vocÃª esteja passando por um momento tÃ£o difÃ­cil e pensando nisso. "
    "Ã‰ muito importante buscar ajuda profissional **imediatamente**. Por favor, entre em contato com o "
    "**CVV (Centro de ValorizaÃ§Ã£o da Vida) ligando para o nÃºmero 188**. A ligaÃ§Ã£o Ã© gratuita "
    "e eles estÃ£o disponÃ­veis 24 horas por dia para conversar com vocÃª de forma sigilosa e segura. "
    "VocÃª nÃ£o estÃ¡ sozinho(a) e hÃ¡ pessoas prontas para te ouvir e ajudar. Por favor, ligue para eles agora."
)

# --- Bloco 7: FunÃ§Ã£o para Inicializar o Modelo ---
@st.cache_resource # Guarda o modelo na memÃ³ria
def init_model(system_instruction_param: str):
    try:
        # MELHORIA: Usando gemini-1.5-flash-latest
        model = genai.GenerativeModel(
            model_name="gemini-1.5-flash-latest",
            generation_config=generation_config,
            safety_settings=safety_settings,
            system_instruction=system_instruction_param
        )
        logger.info(f"Modelo Generativo Gemini (gemini-1.5-flash-latest) inicializado com sucesso com system prompt de {len(system_instruction_param)} caracteres.")
        return model
    except Exception as e:
        logger.critical(f"Erro GRAVE ao carregar o modelo de IA: {e}", exc_info=True)
        st.error(f"Erro grave ao carregar o modelo de IA. O aplicativo nÃ£o pode continuar. Detalhe: {e}")
        st.stop()

model = init_model(system_instruction_aura)

# --- Bloco 8: Gerenciamento do HistÃ³rico da Conversa e BotÃ£o de Reset ---
if SESSION_MESSAGES_KEY in st.session_state and len(st.session_state[SESSION_MESSAGES_KEY]) > 1:
    if st.sidebar.button("ğŸ§¹ Limpar Conversa Atual"): # Movido para a sidebar para menos intrusÃ£o
        st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": "OlÃ¡! Sou Aura. Como vocÃª estÃ¡ se sentindo hoje? (Conversa reiniciada)"}]
        if SESSION_CHAT_KEY in st.session_state:
            del st.session_state[SESSION_CHAT_KEY] # Deleta a sessÃ£o antiga para forÃ§ar recriaÃ§Ã£o
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False # Reseta o feedback
        logger.info("HistÃ³rico da conversa e sessÃ£o do Gemini reiniciados pelo usuÃ¡rio.")
        st.rerun()

if SESSION_MESSAGES_KEY not in st.session_state:
    st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": "OlÃ¡! Sou Aura. Como vocÃª estÃ¡ se sentindo hoje?"}]
    logger.info("HistÃ³rico de mensagens inicializado.")
    st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False


if SESSION_CHAT_KEY not in st.session_state:
    logger.info("Inicializando nova sessÃ£o de chat com Gemini (histÃ³rico serÃ¡ enviado na primeira mensagem).")
    # O histÃ³rico serÃ¡ passado no primeiro send_message da nova funÃ§Ã£o
    # st.session_state[SESSION_CHAT_KEY] = model.start_chat(history=[]) # Alterado para ser tratado na funÃ§Ã£o de envio


# --- Bloco 9: ExibiÃ§Ã£o do HistÃ³rico ---
for message in st.session_state[SESSION_MESSAGES_KEY]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# --- Bloco 10: FunÃ§Ã£o para Enviar Mensagem e Processar Resposta com Streaming (NOVO e MELHORADO) ---
def send_message_to_aura(user_prompt: str):
    """
    Envia a mensagem do usuÃ¡rio para o modelo Gemini, processa a resposta com streaming
    e atualiza o histÃ³rico de mensagens.
    """
    st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    bot_response_final = ""
    try:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response_stream = ""
            with st.spinner("Aura estÃ¡ pensando... ğŸ’¬"):
                # Inicia ou continua a sessÃ£o de chat
                # Se a sessÃ£o jÃ¡ existe, ela continua. Se nÃ£o, start_chat Ã© implicitamente chamado.
                # Passamos o histÃ³rico das mensagens do app para o modelo, exceto a Ãºltima (que Ã© a do assistant placeholder)
                # O Gemini SDK gerencia o histÃ³rico interno da sessÃ£o, mas Ã© bom ser explÃ­cito ao enviar a mensagem.
                current_chat_history_for_model = []
                for msg in st.session_state[SESSION_MESSAGES_KEY][:-1]: # Exclui o placeholder do assistant
                    # Gemini espera 'parts' como uma lista de strings, e o role 'model' para o assistente
                    role_for_gemini = "user" if msg["role"] == "user" else "model"
                    current_chat_history_for_model.append({"role": role_for_gemini, "parts": [msg["content"]]})

                if SESSION_CHAT_KEY not in st.session_state:
                    st.session_state[SESSION_CHAT_KEY] = model.start_chat(history=current_chat_history_for_model)
                    logger.info("Nova sessÃ£o de chat do Gemini iniciada com histÃ³rico.")

                # Envia a mensagem do usuÃ¡rio atual para a sessÃ£o de chat existente
                response_stream = st.session_state[SESSION_CHAT_KEY].send_message(user_prompt, stream=True)


            for chunk in response_stream:
                if chunk.parts: # Gemini 1.5 Flash usa 'parts'
                    for part in chunk.parts:
                        full_response_stream += part.text
                        message_placeholder.markdown(full_response_stream + "â–Œ")
                elif hasattr(chunk, 'text') and chunk.text: # Para compatibilidade ou outros modelos
                    full_response_stream += chunk.text
                    message_placeholder.markdown(full_response_stream + "â–Œ")

                # Checagem de bloqueio por seguranÃ§a no meio do stream
                if chunk.prompt_feedback and chunk.prompt_feedback.block_reason:
                    logger.warning(f"Resposta parcialmente gerada e bloqueada por seguranÃ§a: {chunk.prompt_feedback.block_reason}")
                    full_response_stream += f"\n\n(ConteÃºdo removido por diretrizes de seguranÃ§a: {chunk.prompt_feedback.block_reason_message})"
                    message_placeholder.warning(full_response_stream.strip())
                    break # Sai do loop de streaming

            if not full_response_stream.strip() and not (response_stream.prompt_feedback and response_stream.prompt_feedback.block_reason):
                logger.warning(f"Resposta vazia da IA para o prompt: '{user_prompt}'. Retornando fallback.")
                bot_response_final = "Sinto muito, nÃ£o consegui pensar em uma resposta clara para isso no momento. VocÃª poderia tentar reformular sua pergunta ou falar sobre outra coisa?"
                message_placeholder.markdown(bot_response_final)
            else:
                bot_response_final = full_response_stream.strip()
                message_placeholder.markdown(bot_response_final)

            # Checagem final de bloqueio (caso nenhum texto tenha sido gerado)
            if not bot_response_final and hasattr(response_stream, 'prompt_feedback') and response_stream.prompt_feedback.block_reason:
                block_reason = response_stream.prompt_feedback.block_reason
                block_message = response_stream.prompt_feedback.block_reason_message if hasattr(response_stream.prompt_feedback, 'block_reason_message') else "Motivo nÃ£o especificado."
                logger.error(f"Prompt bloqueado pela API Gemini. RazÃ£o: {block_reason}. Mensagem: {block_message}")
                bot_response_final = f"Sinto muito, sua mensagem nÃ£o pÃ´de ser processada devido Ã s diretrizes de conteÃºdo ({block_reason}). Tente reformular, por favor."
                message_placeholder.error(bot_response_final)


        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": bot_response_final})
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False # Permite novo feedback

    except genai.types.BlockedPromptException as bpe:
        logger.error(f"Prompt bloqueado pela API Gemini (BlockedPromptException): {bpe}", exc_info=True)
        error_msg_user = "Sua mensagem foi bloqueada por nossas diretrizes de seguranÃ§a. Por favor, reformule sua pergunta."
        st.error(error_msg_user)
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": error_msg_user})
    except Exception as e:
        error_msg_user = "Desculpe, ocorreu um problema tÃ©cnico ao processar sua mensagem. Tente novamente mais tarde ou, se o problema persistir, avise o mantenedor do aplicativo."
        logger.error(f"Falha ao enviar mensagem para o Gemini ou processar resposta: {e}", exc_info=True)
        st.error(error_msg_user)
        error_response_for_history = "Sinto muito, tive um problema tÃ©cnico interno e nÃ£o consegui responder. ğŸ˜”"
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": error_response_for_history})
        # O placeholder jÃ¡ terÃ¡ sumido ou o erro jÃ¡ foi mostrado
        with st.chat_message("assistant"):
            st.markdown(error_response_for_history)


# --- Bloco 11: Input e LÃ³gica Principal ---
if prompt := st.chat_input("Digite sua mensagem aqui..."):
    logger.info(f"UsuÃ¡rio enviou: '{prompt[:50]}...' (tamanho: {len(prompt)})") # Loga o inÃ­cio do prompt

    contem_risco = any(regex.search(prompt) for regex in keywords_risco_regex)

    if contem_risco:
        logger.warning(f"Detectada palavra/expressÃ£o de RISCO na mensagem do usuÃ¡rio: '{prompt}'")
        with st.chat_message("assistant"):
            st.warning("**Importante: Se vocÃª estÃ¡ pensando em se machucar ou sente que estÃ¡ em perigo, por favor, busque ajuda profissional imediatamente.**")
            st.markdown(resposta_risco_padrao)
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": prompt}) # Adiciona msg do user ao histÃ³rico
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": resposta_risco_padrao})
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False # Permite novo feedback
    else:
        send_message_to_aura(prompt)

# --- Bloco 12: Coleta de Feedback (NOVO) ---
if len(st.session_state[SESSION_MESSAGES_KEY]) > 1 and st.session_state[SESSION_MESSAGES_KEY][-1]["role"] == "assistant":
    if not st.session_state.get(SESSION_FEEDBACK_SUBMITTED_KEY, False): # Verifica se o feedback jÃ¡ foi dado para a Ãºltima resposta
        st.divider()
        cols = st.columns(8)
        cols[0].write("Essa resposta foi Ãºtil?")
        if cols[1].button("ğŸ‘", key="feedback_positivo"):
            logger.info("Feedback da conversa: Positivo (ğŸ‘)")
            st.toast("Obrigado pelo seu feedback! ğŸ˜Š", icon="ğŸ’–")
            st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = True
            st.rerun() # Para remover os botÃµes de feedback
        if cols[2].button("ğŸ‘", key="feedback_negativo"):
            logger.info("Feedback da conversa: Negativo (ğŸ‘)")
            st.toast("Obrigado pelo seu feedback. Vamos continuar melhorando! ğŸ™", icon="ğŸ’¡")
            st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = True
            st.rerun() # Para remover os botÃµes de feedback


# --- Bloco 13: Recursos Adicionais (NOVO - Exemplo) ---
with st.sidebar: # Movido para a sidebar
    st.markdown("---")
    st.subheader("Recursos Ãšteis ğŸ’¡")
    with st.expander("Para momentos de crise ou necessidade de apoio:", expanded=False):
        st.markdown("- **CVV (Centro de ValorizaÃ§Ã£o da Vida):** Disque **188** (ligaÃ§Ã£o gratuita, 24h).")
        st.markdown("- **SUS:** Procure um CAPS (Centro de AtenÃ§Ã£o Psicossocial) perto de vocÃª.")
        st.markdown("- Lembre-se: VocÃª nÃ£o estÃ¡ sozinho(a). Buscar ajuda Ã© um ato de coragem.")

    with st.expander("Dicas rÃ¡pidas de bem-estar:", expanded=False):
        st.markdown("""
        *   **Pausa e Respire:** Tire alguns minutos para respirar profundamente. Inspire pelo nariz contando atÃ© 4, segure por 4 e expire pela boca contando atÃ© 6.
        *   **Movimente-se:** Uma caminhada leve ou alguns alongamentos podem ajudar a aliviar a tensÃ£o.
        *   **Conecte-se:** Converse com um amigo, familiar ou alguÃ©m de confianÃ§a.
        *   **Pequenos Prazeres:** OuÃ§a uma mÃºsica que vocÃª gosta, assista a algo leve, leia um trecho de um livro.
        *(Lembre-se, estas sÃ£o sugestÃµes simples e nÃ£o substituem o acompanhamento profissional.)*
        """)


# --- Bloco 14: RodapÃ© ---
st.divider()
st.caption(
    "Aura Ã© uma InteligÃªncia Artificial e suas respostas sÃ£o geradas por um modelo de linguagem. "
    "Ela **nÃ£o Ã© uma terapeuta** e nÃ£o substitui o acompanhamento psicolÃ³gico ou psiquiÃ¡trico profissional. "
    "Em caso de emergÃªncia, sofrimento intenso ou necessidade de apoio especializado, **ligue para o CVV (188)** ou procure um profissional de saÃºde mental. "
    "Sua privacidade Ã© importante. As conversas podem ser registradas anonimamente para fins de melhoria do sistema."
)

# --- Fim do app.py ---