# -*- coding: utf-8 -*-
import streamlit as st
import google.generativeai as genai
import time
import re
import logging
import os # Adicionado para depura√ß√£o

# --- Bloco de Configura√ß√£o Inicial (com depura√ß√£o de caminho) ---
# Configura√ß√£o inicial do logger de depura√ß√£o ANTES do logger principal
# para garantir que estas mensagens apare√ßam.
# Este logger √© tempor√°rio para diagn√≥stico.
debug_logger = logging.getLogger("startup_debug")
# Para garantir que as mensagens de debug apare√ßam no console imediatamente
stream_handler_debug = logging.StreamHandler()
formatter_debug = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
stream_handler_debug.setFormatter(formatter_debug)
debug_logger.addHandler(stream_handler_debug)
debug_logger.setLevel(logging.DEBUG) # N√≠vel DEBUG para este logger espec√≠fico

current_working_directory = os.getcwd()
debug_logger.info(f"IN√çCIO DA EXECU√á√ÉO DO SCRIPT.")
debug_logger.info(f"Diret√≥rio de trabalho atual (CWD): {current_working_directory}")
try:
    files_in_cwd = os.listdir(current_working_directory)
    debug_logger.info(f"Arquivos e pastas no CWD ({current_working_directory}): {files_in_cwd}")
except Exception as e_ls:
    debug_logger.error(f"N√£o foi poss√≠vel listar arquivos no CWD ({current_working_directory}): {e_ls}")

# Configura√ß√£o do Logging Principal do Aplicativo (Melhorado e mais detalhado)
# Este logger ser√° usado pelo restante do aplicativo.
logging.basicConfig(
    level=logging.INFO, # Pode ser alterado para DEBUG para ver mais detalhes da app
    format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()
        # logging.FileHandler("aura_app.log", encoding="utf-8") # Descomente para logar em arquivo
    ]
)
logger = logging.getLogger(__name__) # Logger principal para o restante do app

# --- Constantes para Session State ---
SESSION_MESSAGES_KEY = "aura_messages"
SESSION_CHAT_KEY = "aura_chat_session"
SESSION_FEEDBACK_SUBMITTED_KEY = "aura_feedback_submitted"

# --- Bloco 1: Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Aura - Chatbot de Apoio",
    page_icon="üíñ",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- Bloco 2: T√≠tulo e Descri√ß√£o ---
st.title("üíñ Aura: Seu Companheiro Virtual")
st.caption("Um espa√ßo seguro para conversar e encontrar acolhimento. Lembre-se, sou uma IA e n√£o posso substituir um terapeuta profissional.")
st.divider()

# --- Bloco 3: Configura√ß√£o da API Key e System Instruction ---
SYSTEM_PROMPT_FILE = "system_prompt_aura.txt"
system_instruction_aura = ""

# Verifica√ß√£o expl√≠cita se o arquivo existe ANTES de tentar abrir (para depura√ß√£o)
# Usando o logger de depura√ß√£o para estas mensagens
full_path_to_prompt_file = os.path.join(current_working_directory, SYSTEM_PROMPT_FILE)
if not os.path.exists(SYSTEM_PROMPT_FILE): # os.path.exists verifica relativo ao CWD por padr√£o
    debug_logger.error(f"VERIFICA√á√ÉO PR√âVIA: O arquivo '{SYSTEM_PROMPT_FILE}' N√ÉO FOI ENCONTRADO no diret√≥rio de trabalho atual: '{current_working_directory}'.")
    debug_logger.error(f"O sistema tentaria carregar de: '{full_path_to_prompt_file}' (se o nome do arquivo estivesse correto).")
    debug_logger.error(f"Por favor, verifique se o arquivo '{SYSTEM_PROMPT_FILE}' existe EXATAMENTE com este nome e est√° na MESMA PASTA que o script Python.")
    st.error(f"Erro Cr√≠tico de Configura√ß√£o: O arquivo '{SYSTEM_PROMPT_FILE}' n√£o foi encontrado. A Aura pode n√£o funcionar como esperado. Verifique os logs do console para detalhes.")
    # Define um fallback m√≠nimo para que o app n√£o quebre totalmente, mas avise o usu√°rio.
    system_instruction_aura = "Voc√™ √© um chatbot de apoio. Seja gentil e prestativo. Avise que n√£o √© um terapeuta."
    logger.warning(f"Usando instru√ß√£o do sistema de fallback devido √† aus√™ncia de '{SYSTEM_PROMPT_FILE}'.")
else:
    debug_logger.info(f"VERIFICA√á√ÉO PR√âVIA: O arquivo '{SYSTEM_PROMPT_FILE}' FOI ENCONTRADO no diret√≥rio de trabalho atual: '{current_working_directory}'.")
    try:
        with open(SYSTEM_PROMPT_FILE, "r", encoding="utf-8") as f:
            system_instruction_aura = f.read()
        logger.info(f"Instru√ß√£o do sistema '{SYSTEM_PROMPT_FILE}' carregada com sucesso.")
    except Exception as e:
        logger.error(f"Erro ao carregar instru√ß√£o do sistema '{SYSTEM_PROMPT_FILE}' MESMO AP√ìS VERIFICA√á√ÉO: {e}", exc_info=True)
        st.error(f"Erro ao carregar a personalidade da Aura: {e}")
        system_instruction_aura = "Falha ao carregar personalidade. Por favor, avise o desenvolvedor. Sou um chatbot de apoio, seja gentil."
        logger.warning(f"Usando instru√ß√£o do sistema de fallback devido a erro de leitura de '{SYSTEM_PROMPT_FILE}'.")

# Configura√ß√£o da API Key do Google
try:
    GOOGLE_API_KEY_APP = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY_APP)
    logger.info("Chave API do Google configurada com sucesso via Streamlit Secrets.")
except KeyError:
    logger.error("Chave API do Google (GOOGLE_API_KEY) n√£o encontrada nos Secrets do Streamlit.")
    st.error("Ops! Parece que a Chave API do Google n√£o foi configurada nos 'Secrets' do Streamlit. Pe√ßa ajuda para configur√°-la nas defini√ß√µes do app.")
    st.stop()
except Exception as e:
    logger.error(f"Erro inesperado ao configurar a API Key: {e}", exc_info=True)
    st.error(f"Erro inesperado ao configurar a API Key: {e}")
    st.stop()

# --- Bloco 4: Configura√ß√£o do Modelo Gemini ---
generation_config = {
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 800,
}
safety_settings = [
    {"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
    for c in [
        "HARM_CATEGORY_HARASSMENT",
        "HARM_CATEGORY_HATE_SPEECH",
        "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "HARM_CATEGORY_DANGEROUS_CONTENT",
    ]
]

# --- Bloco 6: Defini√ß√µes de Seguran√ßa (CVV) e Detec√ß√£o de Risco Melhorada ---
keywords_risco_originais = [
    "me matar", "me mate", "suicidio", "suic√≠dio",
    "n√£o aguento mais viver", "quero morrer", "queria morrer",
    "quero sumir", "desistir de tudo", "acabar com tudo",
    "fazer mal a mim", "me cortar", "me machucar", "automutila√ß√£o",
    "quero me jogar", "tirar minha vida", "sem esperan√ßa", "fim da linha"
]
keywords_risco_regex = [
    re.compile(r'\b' + re.escape(keyword) + r'\b', re.IGNORECASE)
    for keyword in keywords_risco_originais
]
logger.info(f"{len(keywords_risco_regex)} padr√µes de regex para detec√ß√£o de risco compilados.")

resposta_risco_padrao = (
    "Sinto muito que voc√™ esteja passando por um momento t√£o dif√≠cil e pensando nisso. "
    "√â muito importante buscar ajuda profissional **imediatamente**. Por favor, entre em contato com o "
    "**CVV (Centro de Valoriza√ß√£o da Vida) ligando para o n√∫mero 188**. A liga√ß√£o √© gratuita "
    "e eles est√£o dispon√≠veis 24 horas por dia para conversar com voc√™ de forma sigilosa e segura. "
    "Voc√™ n√£o est√° sozinho(a) e h√° pessoas prontas para te ouvir e ajudar. Por favor, ligue para eles agora."
)

# --- Bloco 7: Fun√ß√£o para Inicializar o Modelo ---
@st.cache_resource
def init_model(system_instruction_param: str):
    # Verifica se a system_instruction_param n√£o est√° vazia ou √© apenas o fallback b√°sico
    if not system_instruction_param or "fallback" in system_instruction_param.lower() or "personalidade" in system_instruction_param.lower():
        logger.warning(f"Inicializando modelo com uma instru√ß√£o de sistema potencialmente problem√°tica/fallback: '{system_instruction_param[:100]}...'")
    try:
        model = genai.GenerativeModel(
            model_name="gemini-1.5-flash-latest",
            generation_config=generation_config,
            safety_settings=safety_settings,
            system_instruction=system_instruction_param
        )
        logger.info(f"Modelo Generativo Gemini (gemini-1.5-flash-latest) inicializado com system prompt de {len(system_instruction_param)} caracteres.")
        return model
    except Exception as e:
        logger.critical(f"Erro GRAVE ao carregar o modelo de IA: {e}", exc_info=True)
        st.error(f"Erro grave ao carregar o modelo de IA. O aplicativo n√£o pode continuar. Detalhe: {e}")
        st.stop()

model = init_model(system_instruction_aura)

# --- Bloco 8: Gerenciamento do Hist√≥rico da Conversa e Bot√£o de Reset ---
if SESSION_MESSAGES_KEY in st.session_state and len(st.session_state[SESSION_MESSAGES_KEY]) > 1:
    if st.sidebar.button("üßπ Limpar Conversa Atual"):
        st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": "Ol√°! Sou Aura. Como voc√™ est√° se sentindo hoje? (Conversa reiniciada)"}]
        if SESSION_CHAT_KEY in st.session_state:
            del st.session_state[SESSION_CHAT_KEY]
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False
        logger.info("Hist√≥rico da conversa e sess√£o do Gemini reiniciados pelo usu√°rio.")
        st.rerun()

if SESSION_MESSAGES_KEY not in st.session_state:
    st.session_state[SESSION_MESSAGES_KEY] = [{"role": "assistant", "content": "Ol√°! Sou Aura. Como voc√™ est√° se sentindo hoje?"}]
    logger.info("Hist√≥rico de mensagens inicializado.")
    st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False

# --- Bloco 9: Exibi√ß√£o do Hist√≥rico ---
for message in st.session_state[SESSION_MESSAGES_KEY]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Bloco 10: Fun√ß√£o para Enviar Mensagem e Processar Resposta com Streaming ---
def send_message_to_aura(user_prompt: str):
    st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    bot_response_final = ""
    try:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response_stream = ""
            
            # Prepara o hist√≥rico para o modelo Gemini
            # O Gemini espera 'parts' como uma lista de strings, e o role 'model' para o assistente
            # A API gerencia o hist√≥rico da sess√£o, mas podemos enviar o contexto atual explicitamente.
            # Para o send_message em uma sess√£o j√° iniciada, geralmente n√£o precisamos reenviar todo o hist√≥rico,
            # mas para a primeira mensagem ap√≥s a limpeza ou in√≠cio, √© crucial.
            # A l√≥gica do SDK do Gemini para model.start_chat(history=...) e chat_session.send_message(...)
            # j√° lida com isso. Apenas garantimos que a sess√£o √© iniciada corretamente.

            if SESSION_CHAT_KEY not in st.session_state:
                # Monta o hist√≥rico inicial para o `start_chat` se a sess√£o n√£o existir
                # Pega todas as mensagens no `st.session_state[SESSION_MESSAGES_KEY]` exceto a √∫ltima (que √© o user_prompt atual)
                initial_history_for_model = []
                # Considera apenas as mensagens que j√° est√£o no hist√≥rico de Streamlit ANTES desta intera√ß√£o.
                # A mensagem do usu√°rio atual (user_prompt) ser√° enviada pelo send_message.
                # O system prompt j√° est√° configurado no modelo.
                # Aqui, o hist√≥rico que o Gemini precisa √© o das intera√ß√µes passadas.
                for msg in st.session_state[SESSION_MESSAGES_KEY][:-1]: # Exclui o prompt atual do usu√°rio
                    role_for_gemini = "user" if msg["role"] == "user" else "model"
                    initial_history_for_model.append({"role": role_for_gemini, "parts": [msg["content"]]})
                
                st.session_state[SESSION_CHAT_KEY] = model.start_chat(history=initial_history_for_model)
                logger.info(f"Nova sess√£o de chat do Gemini iniciada com {len(initial_history_for_model)} mensagens de hist√≥rico.")


            with st.spinner("Aura est√° pensando... üí¨"):
                response_stream = st.session_state[SESSION_CHAT_KEY].send_message(user_prompt, stream=True)

            for chunk in response_stream:
                if chunk.parts:
                    for part in chunk.parts:
                        full_response_stream += part.text
                        message_placeholder.markdown(full_response_stream + "‚ñå")
                elif hasattr(chunk, 'text') and chunk.text:
                    full_response_stream += chunk.text
                    message_placeholder.markdown(full_response_stream + "‚ñå")

                if chunk.prompt_feedback and chunk.prompt_feedback.block_reason:
                    logger.warning(f"Resposta parcialmente gerada e bloqueada por seguran√ßa: {chunk.prompt_feedback.block_reason}")
                    block_message_display = f"\n\n*(Sinto muito, n√£o posso continuar essa resposta devido √†s diretrizes de seguran√ßa. Por favor, tente reformular sua mensagem ou abordar outro t√≥pico.)*"
                    full_response_stream += block_message_display
                    message_placeholder.warning(full_response_stream.strip()) # Usar warning para destacar
                    break

            if not full_response_stream.strip() and not (hasattr(response_stream, 'prompt_feedback') and response_stream.prompt_feedback and response_stream.prompt_feedback.block_reason):
                logger.warning(f"Resposta vazia da IA para o prompt: '{user_prompt}'. Retornando fallback.")
                bot_response_final = "Sinto muito, n√£o consegui pensar em uma resposta clara para isso no momento. Voc√™ poderia tentar reformular sua pergunta ou falar sobre outra coisa?"
                message_placeholder.markdown(bot_response_final)
            else:
                bot_response_final = full_response_stream.strip()
                # Remove o cursor no final se a mensagem n√£o foi bloqueada e cortada
                if not (chunk.prompt_feedback and chunk.prompt_feedback.block_reason):
                     message_placeholder.markdown(bot_response_final)


            if not bot_response_final and hasattr(response_stream, 'prompt_feedback') and response_stream.prompt_feedback.block_reason:
                block_reason = response_stream.prompt_feedback.block_reason
                block_message = getattr(response_stream.prompt_feedback, 'block_reason_message', "Motivo n√£o especificado.")
                logger.error(f"Prompt bloqueado pela API Gemini. Raz√£o: {block_reason}. Mensagem: {block_message}")
                bot_response_final = f"Sinto muito, sua mensagem n√£o p√¥de ser processada devido √†s diretrizes de conte√∫do ({block_reason}). Tente reformular, por favor."
                message_placeholder.error(bot_response_final)

        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": bot_response_final})
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False

    except genai.types.BlockedPromptException as bpe:
        logger.error(f"Prompt bloqueado pela API Gemini (BlockedPromptException): {bpe}", exc_info=True)
        error_msg_user = "Sua mensagem foi bloqueada por nossas diretrizes de seguran√ßa. Por favor, reformule sua pergunta."
        st.error(error_msg_user) # Mostra o erro na UI
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": error_msg_user})
    except Exception as e:
        error_msg_user = "Desculpe, ocorreu um problema t√©cnico ao processar sua mensagem. Tente novamente mais tarde ou, se o problema persistir, avise o mantenedor do aplicativo."
        logger.error(f"Falha ao enviar mensagem para o Gemini ou processar resposta: {e}", exc_info=True)
        # N√£o precisa do st.error(error_msg_user) aqui se o placeholder j√° sumiu
        # A mensagem de erro j√° ser√° adicionada ao chat.
        error_response_for_history = "Sinto muito, tive um problema t√©cnico interno e n√£o consegui responder. üòî"
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": error_response_for_history})
        with st.chat_message("assistant"): # Garante que a mensagem de erro apare√ßa no chat
            st.markdown(error_response_for_history)

# --- Bloco 11: Input e L√≥gica Principal ---
if prompt := st.chat_input("Digite sua mensagem aqui..."):
    logger.info(f"Usu√°rio enviou: '{prompt[:50]}...' (tamanho: {len(prompt)})")

    contem_risco = any(regex.search(prompt) for regex in keywords_risco_regex)

    if contem_risco:
        logger.warning(f"Detectada palavra/express√£o de RISCO na mensagem do usu√°rio: '{prompt}'")
        # Adiciona mensagem do usu√°rio ao hist√≥rico ANTES da resposta de risco
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "user", "content": prompt})
        with st.chat_message("user"): # Exibe a mensagem do usu√°rio
            st.markdown(prompt)

        with st.chat_message("assistant"): # Exibe a resposta de risco
            st.warning("**Importante: Se voc√™ est√° pensando em se machucar ou sente que est√° em perigo, por favor, busque ajuda profissional imediatamente.**")
            st.markdown(resposta_risco_padrao)
        st.session_state[SESSION_MESSAGES_KEY].append({"role": "assistant", "content": resposta_risco_padrao})
        st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = False
    else:
        send_message_to_aura(prompt)

# --- Bloco 12: Coleta de Feedback ---
if len(st.session_state[SESSION_MESSAGES_KEY]) > 1 and st.session_state[SESSION_MESSAGES_KEY][-1]["role"] == "assistant":
    # Garante que a √∫ltima mensagem n√£o seja a de risco ou erro para pedir feedback sobre ela.
    last_aura_message = st.session_state[SESSION_MESSAGES_KEY][-1]["content"]
    is_risk_or_error_message = resposta_risco_padrao in last_aura_message or \
                               "problema t√©cnico interno" in last_aura_message or \
                               "mensagem foi bloqueada" in last_aura_message or \
                               "n√£o p√¥de ser processada" in last_aura_message


    if not is_risk_or_error_message and not st.session_state.get(SESSION_FEEDBACK_SUBMITTED_KEY, False):
        st.divider()
        cols_feedback = st.columns([0.6, 0.2, 0.2], gap="small") # Ajuste para melhor alinhamento
        with cols_feedback[0]:
            st.markdown("<div style='padding-top: 0.5em;'>Essa resposta foi √∫til?</div>", unsafe_allow_html=True)
        if cols_feedback[1].button("üëç", key="feedback_positivo", help="Sim, foi √∫til!"):
            logger.info(f"Feedback da conversa: Positivo (üëç) para a resposta: '{last_aura_message[:100]}...'")
            st.toast("Obrigado pelo seu feedback! üòä", icon="üíñ")
            st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = True
            st.rerun()
        if cols_feedback[2].button("üëé", key="feedback_negativo", help="N√£o, n√£o foi √∫til."):
            logger.info(f"Feedback da conversa: Negativo (üëé) para a resposta: '{last_aura_message[:100]}...'")
            st.toast("Obrigado pelo seu feedback. Vamos continuar melhorando! üôè", icon="üí°")
            st.session_state[SESSION_FEEDBACK_SUBMITTED_KEY] = True
            st.rerun()

# --- Bloco 13: Recursos Adicionais ---
with st.sidebar:
    st.markdown("---")
    st.subheader("Recursos √öteis üí°")
    with st.expander("Para momentos de crise ou necessidade de apoio:", expanded=False):
        st.markdown("- **CVV (Centro de Valoriza√ß√£o da Vida):** Disque **188** (liga√ß√£o gratuita, 24h).")
        st.markdown("- **SUS:** Procure um CAPS (Centro de Aten√ß√£o Psicossocial) perto de voc√™.")
        st.markdown("- Lembre-se: Voc√™ n√£o est√° sozinho(a). Buscar ajuda √© um ato de coragem.")

    with st.expander("Dicas r√°pidas de bem-estar:", expanded=False):
        st.markdown("""
        *   **Pausa e Respire:** Tire alguns minutos para respirar profundamente. Inspire pelo nariz contando at√© 4, segure por 4 e expire pela boca contando at√© 6.
        *   **Movimente-se:** Uma caminhada leve ou alguns alongamentos podem ajudar a aliviar a tens√£o.
        *   **Conecte-se:** Converse com um amigo, familiar ou algu√©m de confian√ßa.
        *   **Pequenos Prazeres:** Ou√ßa uma m√∫sica que voc√™ gosta, assista a algo leve, leia um trecho de um livro.
        *(Lembre-se, estas s√£o sugest√µes simples e n√£o substituem o acompanhamento profissional.)*
        """)

# --- Bloco 14: Rodap√© ---
st.divider()
st.caption(
    "Aura √© uma Intelig√™ncia Artificial e suas respostas s√£o geradas por um modelo de linguagem. "
    "Ela **n√£o √© uma terapeuta** e n√£o substitui o acompanhamento psicol√≥gico ou psiqui√°trico profissional. "
    "Em caso de emerg√™ncia, sofrimento intenso ou necessidade de apoio especializado, **ligue para o CVV (188)** ou procure um profissional de sa√∫de mental. "
    "Sua privacidade √© importante. As conversas podem ser registradas anonimamente para fins de melhoria do sistema."
)
debug_logger.info("FIM DA EXECU√á√ÉO DO SCRIPT.")
# --- Fim do app.py ---
